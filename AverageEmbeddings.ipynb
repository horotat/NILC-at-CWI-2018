{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, AdaBoostClassifier, AdaBoostRegressor, ExtraTreesClassifier, ExtraTreesRegressor\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "from data import Data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = ['datasets/english/News_Train.tsv',\n",
    "                 'datasets/english/WikiNews_Train.tsv',\n",
    "                 'datasets/english/Wikipedia_Train.tsv',\n",
    "                ]\n",
    "dev_data = ['datasets/english/News_Dev.tsv',\n",
    "            'datasets/english/WikiNews_Dev.tsv',\n",
    "            'datasets/english/Wikipedia_Dev.tsv'\n",
    "           ]\n",
    "\n",
    "data_train = Data()\n",
    "data_train.load_data(training_data)\n",
    "\n",
    "data_dev = Data()\n",
    "data_dev.load_data(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "    './glove.100d.bin',\n",
    "    binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# AUC for a binary classifier\n",
    "def auc(y_true, y_pred):   \n",
    "    ptas = tf.stack([binary_PTA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)\n",
    "    pfas = tf.stack([binary_PFA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)\n",
    "    pfas = tf.concat([tf.ones((1,)) ,pfas],axis=0)\n",
    "    binSizes = -(pfas[1:]-pfas[:-1])\n",
    "    s = ptas*binSizes\n",
    "    return K.sum(s, axis=0)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# PFA, prob false alert for binary classifier\n",
    "def binary_PFA(y_true, y_pred, threshold=K.variable(value=0.5)):\n",
    "    y_pred = K.cast(y_pred >= threshold, 'float32')\n",
    "    # N = total number of negative labels\n",
    "    N = K.sum(1 - y_true)\n",
    "    # FP = total number of false alerts, alerts from the negative class labels\n",
    "    FP = K.sum(y_pred - y_pred * y_true)    \n",
    "    return FP/N\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "# P_TA prob true alerts for binary classifier\n",
    "def binary_PTA(y_true, y_pred, threshold=K.variable(value=0.5)):\n",
    "    y_pred = K.cast(y_pred >= threshold, 'float32')\n",
    "    # P = total number of positive labels\n",
    "    P = K.sum(y_true)\n",
    "    # TP = total number of correct alerts, alerts from the positive class labels\n",
    "    TP = K.sum(y_pred * y_true)    \n",
    "    return TP/P\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "model_nn = Sequential()\n",
    "\n",
    "\n",
    "get_custom_objects().update({'swish': Activation(swish )})\n",
    "\n",
    "model_nn.add(Dense(100, input_shape=(100,), activation='relu'))\n",
    "model_nn.add(Dense(100, input_shape=(100,), activation='relu'))\n",
    "model_nn.add(Dense(1, activation='sigmoid'))\n",
    "model_nn.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=[auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(data, embeddings):\n",
    "    data_embeddings = []\n",
    "    for index, instance in enumerate(data.instances):\n",
    "        words = []\n",
    "        for i in instance.target:\n",
    "            if instance.tokens[i] in embeddings:\n",
    "                words.append(embeddings[instance.tokens[i]])\n",
    "        if len(words) == 0:\n",
    "            words.append(embeddings['unk'])\n",
    "        data_embeddings.append(np.average(words, axis=0))\n",
    "    print(len(data_embeddings))\n",
    "    print(len(data_embeddings[0]))\n",
    "    return np.asarray(data_embeddings, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27299\n",
      "100\n",
      "3328\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "average_embedding_train = get_data(data_train, model)\n",
    "average_embedding_dev = get_data(data_dev, model)\n",
    "y_train = np.array([instance.label[0] for instance in data_train.instances])\n",
    "y_dev = np.array([instance.label[0] for instance in data_dev.instances])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27299 samples, validate on 3328 samples\n",
      "Epoch 1/10\n",
      "27299/27299 [==============================] - 27s - loss: 0.5240 - auc: 0.8083 - val_loss: 0.4874 - val_auc: 0.8416\n",
      "Epoch 2/10\n",
      "27299/27299 [==============================] - 15s - loss: 0.4788 - auc: nan - val_loss: 0.4821 - val_auc: 0.8448\n",
      "Epoch 3/10\n",
      "27299/27299 [==============================] - 13s - loss: 0.4594 - auc: nan - val_loss: 0.4676 - val_auc: 0.8572\n",
      "Epoch 4/10\n",
      "27299/27299 [==============================] - 13s - loss: 0.4448 - auc: nan - val_loss: 0.4776 - val_auc: 0.8539\n",
      "Epoch 5/10\n",
      "27299/27299 [==============================] - 13s - loss: 0.4339 - auc: 0.8785 - val_loss: 0.4630 - val_auc: 0.8632\n",
      "Epoch 6/10\n",
      "27299/27299 [==============================] - 13s - loss: 0.4213 - auc: 0.8846 - val_loss: 0.4434 - val_auc: 0.8680\n",
      "Epoch 7/10\n",
      "27299/27299 [==============================] - 13s - loss: 0.4113 - auc: nan - val_loss: 0.4527 - val_auc: 0.8697\n",
      "Epoch 8/10\n",
      "27299/27299 [==============================] - 13s - loss: 0.4047 - auc: 0.8959 - val_loss: 0.4531 - val_auc: 0.8730\n",
      "Epoch 9/10\n",
      "27299/27299 [==============================] - 13s - loss: 0.3965 - auc: 0.8999 - val_loss: 0.4393 - val_auc: 0.8786\n",
      "Epoch 10/10\n",
      "27299/27299 [==============================] - 14s - loss: 0.3881 - auc: 0.9042 - val_loss: 0.4556 - val_auc: 0.8734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1f30ab8da0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nn.fit(\n",
    "    average_embedding_train,\n",
    "    y_train,\n",
    "    validation_data=(average_embedding_dev, y_dev),\n",
    "    epochs=10,\n",
    "    batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_nn.save('./model_2relu_sigmoid.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
